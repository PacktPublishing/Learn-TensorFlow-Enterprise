{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucRU_yVvUYxj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import IPython.display as display\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "VLD5Q76iU1n7",
    "outputId": "6c3c0cd3-9445-4267-8b17-7f001608e95c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(nsl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "N_cHJLawUf0b",
    "outputId": "5aa69698-2420-477f-d868-766619b873c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fbCRxK-5UwAI",
    "outputId": "3c6a0a19-0294-4a30-e9a5-c228e0a7a8ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qySiqOx2UyIh"
   },
   "outputs": [],
   "source": [
    "root_dir = './tfrecord-dataset/flowers'\n",
    "train_file_pattern = \"{}/image_classification_builder-train*.tfrecord*\".format(root_dir)\n",
    "val_file_pattern = \"{}/image_classification_builder-validation*.tfrecord*\".format(root_dir)\n",
    "test_file_pattern = \"{}/image_classification_builder-test*.tfrecord*\".format(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npF2JubpU-G8"
   },
   "outputs": [],
   "source": [
    "train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))\n",
    "val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))\n",
    "test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEYZHLMfVAmU"
   },
   "outputs": [],
   "source": [
    "train_all_ds = tf.data.TFRecordDataset(train_all_files, num_parallel_reads = AUTOTUNE)\n",
    "val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads = AUTOTUNE)\n",
    "test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoQYySJHT0YI"
   },
   "outputs": [],
   "source": [
    "train_sample_size = sum(1 for _ in tf.data.TFRecordDataset(train_all_files))\n",
    "validation_sample_size = sum(1 for _ in tf.data.TFRecordDataset(val_all_files))\n",
    "test_sample_size = sum(1 for _ in tf.data.TFRecordDataset(test_all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "wQdfHbfqVFYa",
    "outputId": "862b4c36-79c8-4ffd-b93e-6bcadf2141d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for training: 3540 \n",
      " Sample size for validation: 80 \n",
      " Sample size for test: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample size for training: {0}\".format(train_sample_size)\n",
    "     ,'\\n', \"Sample size for validation: {0}\".format(validation_sample_size)\n",
    "     ,'\\n', \"Sample size for test: {0}\".format(test_sample_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3w-MX0NVM2z"
   },
   "source": [
    "## Transforming TFRecords for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGaerMgmVIAO"
   },
   "outputs": [],
   "source": [
    "def decode_and_resize(serialized_example):\n",
    "    # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n",
    "    # label is integer index of class.\n",
    "    \n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "      serialized_example,\n",
    "      features = {\n",
    "    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/text' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/filename' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/format' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/height' : tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width' : tf.io.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "    image = tf.io.decode_jpeg(parsed_features['image/encoded'], channels=3)\n",
    "    label = tf.cast(parsed_features['image/class/label'], tf.int32)\n",
    "    label_txt = tf.cast(parsed_features['image/class/text'], tf.string)\n",
    "    label_one_hot = tf.one_hot(label, depth = 5)\n",
    "    resized_image = tf.image.resize(image, [224, 224], method='nearest')\n",
    "    return resized_image, label_one_hot\n",
    "\n",
    "def normalize(image, label):\n",
    "    #Convert `image` from [0, 255] -> [0, 1.0] floats \n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od2MfYzBVTi-"
   },
   "outputs": [],
   "source": [
    "resized_train_ds = train_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "resized_val_ds = val_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "resized_test_ds = test_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "resized_normalized_train_ds = resized_train_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "resized_normalized_val_ds = resized_val_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "resized_normalized_test_ds = resized_test_ds.map(normalize, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUBIHeiKVWFK"
   },
   "outputs": [],
   "source": [
    "pixels =224\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = validation_sample_size\n",
    "TEST_BATCH_SIZE = test_sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZOVaOjwVaQJ"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "#prepped_test_ds = prepare_for_model(resized_normalized_test_ds, TEST_BATCH_SIZE, False, False)\n",
    "\n",
    "prepped_test_ds = resized_normalized_test_ds.batch(TEST_BATCH_SIZE).prefetch(buffer_size = AUTOTUNE)\n",
    "\n",
    "prepped_train_ds = resized_normalized_train_ds.repeat(100).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "prepped_train_ds = prepped_train_ds.batch(TRAIN_BATCH_SIZE)\n",
    "prepped_train_ds = prepped_train_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "\n",
    "prepped_val_ds = resized_normalized_val_ds.repeat(NUM_EPOCHS).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "prepped_val_ds = prepped_val_ds.batch(80)\n",
    "prepped_val_ds = prepped_val_ds.prefetch(buffer_size = AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjI-9EoVVcxX"
   },
   "outputs": [],
   "source": [
    "train_image_batch, train_label_batch = next(iter(prepped_train_ds))\n",
    "val_image_batch, val_label_batch = next(iter(prepped_val_ds))\n",
    "test_image_batch, test_label_batch = next(iter(prepped_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUzjWuA5Vhmu"
   },
   "source": [
    "Now all training image batch should be [N, 224, 224, 3], all label batch should be [N, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYBnm2F-zVrd"
   },
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/text' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/filename' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/format' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/height' : tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width' : tf.io.FixedLenFeature([], tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "parsd_ds = train_all_ds.map(_parse_function)\n",
    "\n",
    "label_map = {}\n",
    "# getting label mapping\n",
    "for image_features in parsd_ds.shuffle(1024).take(100):\n",
    "    label_idx = image_features['image/class/label'].numpy()\n",
    "    label_str = image_features['image/class/text'].numpy().decode()\n",
    "    if label_idx not in label_map:\n",
    "        label_map[label_idx] = label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9oJLUX260Buz",
    "outputId": "00566ea5-7d4e-4489-e782-fdcec78e5b07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'roses', 1: 'sunflowers', 2: 'daisy', 3: 'dandelion', 4: 'tulips'}"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVryTeTRVezd"
   },
   "outputs": [],
   "source": [
    "FINE_TUNING_CHOICE = False\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define L1 and L2 regularizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVnx0hD7049c"
   },
   "outputs": [],
   "source": [
    "KERNEL_REGULARIZER = tf.keras.regularizers.l2(l=0.01)\n",
    "ACTIVITY_REGULARIZER = tf.keras.regularizers.L1L2(l1=0.01,l2=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass regularizers into model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVTSc4W9VkCZ"
   },
   "outputs": [],
   "source": [
    "mdl = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\",\n",
    "                   trainable=FINE_TUNING_CHOICE), \n",
    "    tf.keras.layers.Dense(NUM_CLASSES \n",
    "                          ,activation='softmax'\n",
    "                          ,kernel_regularizer=KERNEL_REGULARIZER\n",
    "                          ,activity_regularizer = ACTIVITY_REGULARIZER\n",
    "                          ,name = 'custom_class')\n",
    "])\n",
    "mdl.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "DaWf8VoCVoox",
    "outputId": "fd1269a2-36a2-4c82-9dd9-efea5efdcfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "custom_class (Dense)         (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 23,575,045\n",
      "Trainable params: 10,245\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp80MC-XVs8K"
   },
   "outputs": [],
   "source": [
    "mdl.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeGCDPmZVvVr"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"mymodel_{epoch}\",\n",
    "        save_best_only=False,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_accuracy\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RwDJZrX5V2hE",
    "outputId": "45427842-9a8c-4d1e-c3f7-e59f733c96cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 224, 224, 3), (None, 5)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepped_train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNLIo0ViV7pL"
   },
   "source": [
    "`prepped_train_ds` is a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKuIzu7VhGmB"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "pUQep6U4V4f2",
    "outputId": "f60da63d-a0ba-40fd-d3fd-709a38ebc0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 1 steps\n",
      "Epoch 1/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.3825 - accuracy: 0.7033\n",
      "Epoch 00001: saving model to mymodel_1\n",
      "INFO:tensorflow:Assets written to: mymodel_1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel_1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 19s 192ms/step - loss: 1.3818 - accuracy: 0.7047 - val_loss: 1.3006 - val_accuracy: 0.8125\n",
      "Epoch 2/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.2354 - accuracy: 0.8608\n",
      "Epoch 00002: saving model to mymodel_2\n",
      "INFO:tensorflow:Assets written to: mymodel_2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel_2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 15s 146ms/step - loss: 1.2354 - accuracy: 0.8603 - val_loss: 1.2103 - val_accuracy: 0.8875\n",
      "Epoch 3/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.2081 - accuracy: 0.8876\n",
      "Epoch 00003: saving model to mymodel_3\n",
      "INFO:tensorflow:Assets written to: mymodel_3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel_3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 14s 144ms/step - loss: 1.2091 - accuracy: 0.8866 - val_loss: 1.2177 - val_accuracy: 0.8375\n",
      "Epoch 4/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.1924 - accuracy: 0.8961\n",
      "Epoch 00004: saving model to mymodel_4\n",
      "INFO:tensorflow:Assets written to: mymodel_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel_4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 14s 143ms/step - loss: 1.1917 - accuracy: 0.8969 - val_loss: 1.1592 - val_accuracy: 0.9250\n",
      "Epoch 5/5\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 1.1755 - accuracy: 0.9097\n",
      "Epoch 00005: saving model to mymodel_5\n",
      "INFO:tensorflow:Assets written to: mymodel_5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mymodel_5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "100/100 [==============================] - 14s 141ms/step - loss: 1.1757 - accuracy: 0.9094 - val_loss: 1.1665 - val_accuracy: 0.9000\n",
      "Total training time with regularization in second:  76.6740140914917\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "mdl.fit(\n",
    "    prepped_train_ds,\n",
    "    epochs=5, steps_per_epoch=100,\n",
    "    validation_data=prepped_val_ds,\n",
    "    validation_steps=1,\n",
    "    callbacks=callbacks)\n",
    "toc = time.time()\n",
    "print('Total training time with regularization in second: ', toc-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRCd9r9jk9Kp"
   },
   "source": [
    "Training process with L1 and L2 regularization is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "L1L2Regularization_GCS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
