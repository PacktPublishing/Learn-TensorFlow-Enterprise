{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4I8UrzQcy5cL",
    "outputId": "62dec8af-33f8-4169-bea8-b2b6b9ca06dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../chapter_07/train_base_model/tf_datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "B4w09Uy2zxlJ",
    "outputId": "a7f7d096-c344-4323-f216-773e89855383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 182640\n",
      "-rw-r--r-- 1 root root      782 Aug  6 02:23 dataset_info.json\n",
      "-rw-r--r-- 1 root root  3417952 Aug  6 02:23 image_classification_builder-validation.tfrecord-00000-of-00001\n",
      "-rw-r--r-- 1 root root  2419057 Aug  6 02:23 image_classification_builder-test.tfrecord-00000-of-00001\n",
      "-rw-r--r-- 1 root root       49 Aug  6 02:23 image-encoded.image.json\n",
      "-rw-r--r-- 1 root root 90482995 Aug  6 02:24 image_classification_builder-train.tfrecord-00000-of-00002\n",
      "-rw-r--r-- 1 root root 90701075 Aug  6 02:24 image_classification_builder-train.tfrecord-00001-of-00002\n",
      "drwxr-xr-x 1 root root        0 Oct  5 18:14 custom_cnn\n",
      "drwxr-xr-x 1 root root        0 Oct  5 18:14 full_model\n",
      "drwxr-xr-x 1 root root        0 Oct  5 18:14 quantized_resnet_vector\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt /content/tfrecord-dataset/flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "yffJmo1iz0dH",
    "outputId": "e30856f6-1c77-4945-d65f-36813ba010b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "0.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import IPython.display as display\n",
    "from tensorflow import keras\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "print(tf.__version__)\n",
    "print(hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9CSGBv3iz5sJ"
   },
   "outputs": [],
   "source": [
    "root_dir = '/content/tfrecord-dataset/flowers'\n",
    "train_file_pattern = \"{}/image_classification_builder-train*.tfrecord*\".format(root_dir)\n",
    "val_file_pattern = \"{}/image_classification_builder-validation*.tfrecord*\".format(root_dir)\n",
    "test_file_pattern = \"{}/image_classification_builder-test*.tfrecord*\".format(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "u2K_D7yT0Evc"
   },
   "outputs": [],
   "source": [
    "train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))\n",
    "val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))\n",
    "test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_file_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zvKvYdRe0Now"
   },
   "outputs": [],
   "source": [
    "train_all_ds = tf.data.TFRecordDataset(train_all_files, num_parallel_reads = AUTOTUNE)\n",
    "val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads = AUTOTUNE)\n",
    "test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "zbzrLzM30QkV",
    "outputId": "2335b00b-f425-4e46-e126-d3fb841ca64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for training: 3540 \n",
      " Sample size for validation: 80 \n",
      " Sample size for test: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample size for training: {0}\".format(sum(1 for _ in tf.data.TFRecordDataset(train_all_files)))\n",
    "     ,'\\n', \"Sample size for validation: {0}\".format(sum(1 for _ in tf.data.TFRecordDataset(val_all_files)))\n",
    "     ,'\\n', \"Sample size for test: {0}\".format(sum(1 for _ in tf.data.TFRecordDataset(test_all_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6dUSbpI00TsJ"
   },
   "outputs": [],
   "source": [
    "def decode_and_resize(serialized_example):\n",
    "    # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n",
    "    # label is integer index of class.\n",
    "    \n",
    "    parsed_features = tf.io.parse_single_example(\n",
    "      serialized_example,\n",
    "      features = {\n",
    "    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/class/text' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/encoded' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/filename' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/format' : tf.io.FixedLenFeature([], tf.string),\n",
    "    'image/height' : tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image/width' : tf.io.FixedLenFeature([], tf.int64)\n",
    "    })\n",
    "    image = tf.io.decode_jpeg(parsed_features['image/encoded'], channels=3)\n",
    "    label = tf.cast(parsed_features['image/class/label'], tf.int32)\n",
    "    label_txt = tf.cast(parsed_features['image/class/text'], tf.string)\n",
    "    label_one_hot = tf.one_hot(label, depth = 5)\n",
    "    resized_image = tf.image.resize(image, [224, 224], method='nearest')\n",
    "    return resized_image, label_one_hot\n",
    "\n",
    "def normalize(image, label):\n",
    "    #Convert `image` from [0, 255] -> [0, 1.0] floats \n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0V9uRDnIfWSK"
   },
   "outputs": [],
   "source": [
    "resized_train_ds = train_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "resized_val_ds = val_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "resized_test_ds = test_all_ds.map(decode_and_resize, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "resized_normalized_train_ds = resized_train_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "resized_normalized_val_ds = resized_val_ds.map(normalize, num_parallel_calls=AUTOTUNE)\n",
    "resized_normalized_test_ds = resized_test_ds.map(normalize, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FJ63ZOn0ZDoy"
   },
   "outputs": [],
   "source": [
    "pixels =224\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "# Validation and test data are small. Use all in a batch.\n",
    "VAL_BATCH_SIZE = sum(1 for _ in tf.data.TFRecordDataset(val_all_files))\n",
    "TEST_BATCH_SIZE = sum(1 for _ in tf.data.TFRecordDataset(test_all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MUpEn5ECdIR0"
   },
   "outputs": [],
   "source": [
    "def prepare_for_model(ds, BATCH_SIZE, cache=True, TRAINING_DATA=True, shuffle_buffer_size=1000):\n",
    "  # This is a small dataset, only load it once, and keep it in memory.\n",
    "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "  # fit in memory.\n",
    "  if cache:\n",
    "    if isinstance(cache, str):\n",
    "      ds = ds.cache(cache)\n",
    "    else:\n",
    "      ds = ds.cache()\n",
    "  \n",
    "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "  if TRAINING_DATA:\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "\n",
    "  ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vN8Y9TwEdQGt"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "prepped_test_ds = prepare_for_model(resized_normalized_test_ds, TEST_BATCH_SIZE, False, False)\n",
    "\n",
    "prepped_train_ds = resized_normalized_train_ds.repeat(100).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "prepped_train_ds = prepped_train_ds.batch(TRAIN_BATCH_SIZE)\n",
    "prepped_train_ds = prepped_train_ds.prefetch(buffer_size = AUTOTUNE)\n",
    "\n",
    "\n",
    "prepped_val_ds = resized_normalized_val_ds.repeat(NUM_EPOCHS).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "prepped_val_ds = prepped_val_ds.batch(80)\n",
    "prepped_val_ds = prepped_val_ds.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jlYd5VcBfOZr"
   },
   "outputs": [],
   "source": [
    "FINE_TUNING_CHOICE = False\n",
    "NUM_CLASSES = 5\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k0Zgd9hbfxfC"
   },
   "outputs": [],
   "source": [
    "mdl = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,), name='input_layer'),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/4\",\n",
    "                   trainable=FINE_TUNING_CHOICE, name = 'resnet_fv'), \n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax', name = 'custom_class')\n",
    "])\n",
    "mdl.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j9uRAS7Bf4OP"
   },
   "outputs": [],
   "source": [
    "mdl.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "DJaFssZVf60z",
    "outputId": "eedaf578-51c1-4510-9e14-02c85032e12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 1.2731 - accuracy: 0.6772 - val_loss: 1.0868 - val_accuracy: 0.9125\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 15s 146ms/step - loss: 1.1148 - accuracy: 0.8697 - val_loss: 1.0934 - val_accuracy: 0.8875\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 15s 148ms/step - loss: 1.0843 - accuracy: 0.9053 - val_loss: 1.1047 - val_accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 15s 150ms/step - loss: 1.0786 - accuracy: 0.9097 - val_loss: 1.0632 - val_accuracy: 0.8875\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 15s 152ms/step - loss: 1.0636 - accuracy: 0.9272 - val_loss: 1.0855 - val_accuracy: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f17ad637358>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl.fit(\n",
    "    prepped_train_ds,\n",
    "    epochs=5, steps_per_epoch=100,\n",
    "    validation_data=prepped_val_ds,\n",
    "    validation_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "opBQW5O0hI9c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Packt_chapter_08_tfrecord.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
